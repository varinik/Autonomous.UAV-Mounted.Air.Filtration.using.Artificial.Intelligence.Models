{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfjMkKektK_F"
      },
      "outputs": [],
      "source": [
        "# old\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import scipy.spatial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.optim import Adam\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0TQ9cDgmAld"
      },
      "source": [
        "**Model Building and Training**\n",
        "\n",
        "*Source Approximation*\n",
        "\n",
        "One model used for source approximation: a Long Short Term Memory Model (LSTM). This LSTM is separate from the one used in the trajectory prediction.\n",
        "\n",
        "*Trajectory Predication*\n",
        "\n",
        "Two models used for trajectory prediction: a Long Short Term Memory Model(LSTM) and a Hidden Markov Model(HMM).\n",
        "\n",
        "The LSTM was built using the PyTorch library and the HMM is a custom built model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKLiQmuCnDId"
      },
      "source": [
        "SOURCE APPROXIMATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VHz3F6dAl7u3"
      },
      "outputs": [],
      "source": [
        "# Building and training the LSTM\n",
        "\n",
        "def euclidean_distance_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(y_true - y_pred), axis=1) + 1e-6))\n",
        "\n",
        "def generate_random_points(x0, y0, z0):\n",
        "    x = [0, 0, 0]\n",
        "    y = [0, 0, 0]\n",
        "    z = [0, 0, 0]\n",
        "\n",
        "    for i in range(3):\n",
        "        x[i] = x0 + np.random.uniform(0,1)\n",
        "    for i in range(3):\n",
        "        y[i] = y0 + np.random.uniform(0,1)\n",
        "    for i in range(3):\n",
        "        z[i] = z0 + np.random.uniform(0.01,0.1)\n",
        "\n",
        "    return [[x[0], y[0], z[0]], [x[1], y[1], z[1]], [x[2], y[2], z[2]]]\n",
        "\n",
        "# Function to generate synthetic data\n",
        "def generate_data(num_samples=500):\n",
        "    X_train, Y_train = [], []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        x0, y0, z0 = np.random.randint(0, 50, 3)  # Source location\n",
        "\n",
        "        # Generate 3 observation points\n",
        "        points = generate_random_points(x0, y0, z0)\n",
        "        concentrations = []\n",
        "\n",
        "        for x, y, z in points:\n",
        "\n",
        "            # Constants\n",
        "            x_factor = 2\n",
        "            u = 1  # Wind speed\n",
        "            H = 0\n",
        "            ay, by = 1, 0.92\n",
        "            az, bz = 1, 0.87\n",
        "\n",
        "            # Calculate s_y and s_z\n",
        "            s_y = 1 * ay * abs(x - x0) ** by\n",
        "            s_z = 1 * az * abs(x - x0) ** bz\n",
        "\n",
        "            # Take care of divide by zero by checking s_y and s_z\n",
        "            if s_y == 0:\n",
        "                s_y = 0.0002\n",
        "            if s_z == 0:\n",
        "                s_z = 0.0002\n",
        "\n",
        "            # Calculate the exponential terms\n",
        "            EXP1 = -(y - y0) ** 2 / (s_y ** 2)\n",
        "            EXP2 = -(z - z0 - H) ** 2 / (2 * s_z ** 2)\n",
        "            EXP3 = -(z - z0 + H) ** 2 / (2 * s_z ** 2)\n",
        "\n",
        "            # Calculate concentration\n",
        "            concentration = 100 * 1 / (2 * np.pi * s_y * s_z * u)\n",
        "\n",
        "            # Check if x is less than x0 (if so, concentration becomes 0)\n",
        "            if x < x0:\n",
        "                concentration = 0\n",
        "\n",
        "            # Calculate concentration based on the exponential terms\n",
        "            concentration *= np.exp(EXP1)\n",
        "            concentration *= np.exp(EXP2) + np.exp(EXP3)\n",
        "\n",
        "            # Max concentration to 300\n",
        "            if concentration > 300:\n",
        "                concentration = 300\n",
        "\n",
        "\n",
        "            concentrations.append(concentration)\n",
        "\n",
        "        input_features = np.column_stack((points, concentrations))\n",
        "        X_train.append(input_features)\n",
        "        Y_train.append([x0, y0, z0])\n",
        "    return np.array(X_train), np.array(Y_train)\n",
        "\n",
        "# Generate data using the Gaussian Plume Equation and split data\n",
        "X, Y = generate_data()\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "source = keras.Sequential([\n",
        "    layers.LSTM(128, return_sequences=True, input_shape=(3, 4)),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(3)\n",
        "])\n",
        "\n",
        "# compile model using adam optimizer and euclidean distance as a loss function\n",
        "source.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=euclidean_distance_loss)\n",
        "\n",
        "# Train the model\n",
        "history = source.fit(X_train, Y_train, epochs=51, batch_size=64, validation_data=(X_val, Y_val), verbose=1)\n",
        "\n",
        "# # Plot training and validation loss\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# plt.plot(history.history['loss'], label='Train Loss')\n",
        "# plt.plot(history.history['val_loss'], linestyle='dashed', label='Val Loss')\n",
        "# plt.xlabel(\"Epochs\")\n",
        "# plt.ylabel(\"Loss (Euclidean Distance)\")\n",
        "# plt.title(\"Training & Validation Loss\")\n",
        "# plt.legend()\n",
        "# plt.grid()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or insert previously trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "source = load_model('models/trajectory_model.keras', custom_objects={'euclidean_distance_loss': euclidean_distance_loss}) # replace with location of file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkAwu7dhnF3b"
      },
      "source": [
        "TRAJECTORY PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3EA3IcsenAXC"
      },
      "outputs": [],
      "source": [
        "# Building HMM\n",
        "\n",
        "class HMM3DWithViterbi:\n",
        "    def __init__(self, n_states, nn_hidden_layers=50, max_iter=1000):\n",
        "        self.n_states = n_states\n",
        "\n",
        "        # uniform transition probabilities\n",
        "        self.transition_matrix = np.ones((n_states, n_states)) / n_states\n",
        "\n",
        "        # Uniform initial probabilities\n",
        "        self.initial_probs = np.ones(n_states) / n_states\n",
        "\n",
        "        #MLPRegressor used for non-linear emmission probabilities\n",
        "        self.nn_models = [MLPRegressor(hidden_layer_sizes=nn_hidden_layers, max_iter=max_iter) for _ in range(n_states)]  # One NN per state\n",
        "\n",
        "    # implements the viterbi algorithm to find the most probable sequence of hidden states\n",
        "    def viterbi(self, observations):\n",
        "        # number of observations in the path\n",
        "        n_observations = len(observations)\n",
        "        # stores the log-probabilities of all the paths that end in hidden state j at time t\n",
        "        log_probs = np.zeros((n_observations, self.n_states))\n",
        "        # stores the best path\n",
        "        paths = np.zeros((n_observations, self.n_states), dtype=int)\n",
        "\n",
        "        # defining the probabilities for the first observation\n",
        "        emission_probs = self._emission_probabilities(observations[0])\n",
        "        log_probs[0] = np.log(self.initial_probs + 1e-9) + np.log(emission_probs + 1e-9)\n",
        "\n",
        "        # loop to go through all possible paths and find the best one based on the emission probabilities\n",
        "        for t in range(1, n_observations):\n",
        "            for j in range(self.n_states):\n",
        "                transition_probs = log_probs[t - 1] + np.log(self.transition_matrix[:, j] + 1e-9)\n",
        "                best_prev_state = np.argmax(transition_probs)\n",
        "                log_probs[t, j] = transition_probs[best_prev_state] + np.log(self._emission_probabilities(observations[t])[j] + 1e-9)\n",
        "                paths[t, j] = best_prev_state\n",
        "\n",
        "        # backtracking to give the most probable path\n",
        "        best_last_state = np.argmax(log_probs[-1])\n",
        "        best_path = [best_last_state]\n",
        "        for t in range(n_observations - 1, 0, -1):\n",
        "            best_last_state = paths[t, best_last_state]\n",
        "            best_path.append(best_last_state)\n",
        "        return list(reversed(best_path))\n",
        "\n",
        "    # function to train the model\n",
        "    def fit(self, all_observations):\n",
        "        # preparing the data\n",
        "        X_by_state = [[] for _ in range(self.n_states)]\n",
        "        y_by_state = [[] for _ in range(self.n_states)]\n",
        "        sum = 0\n",
        "        total = 0\n",
        "        all_data = np.vstack(all_observations)\n",
        "        # kmeans clustering used to see how many hidden states are needed for the specific data\n",
        "        kmeans = KMeans(n_clusters=self.n_states, n_init=10).fit(all_data)\n",
        "        initial_states = kmeans.labels_\n",
        "\n",
        "        # associates parts of the trajectory with certain hidden states\n",
        "        for trajectory in all_observations:\n",
        "            states = self.viterbi(trajectory)[:-2]\n",
        "            for t, state in enumerate(states):\n",
        "                X_by_state[state].append(trajectory[t])\n",
        "                y_by_state[state].append(trajectory[t + 1])\n",
        "\n",
        "        # trains the emission probabilties model using the data\n",
        "        for state in range(self.n_states):\n",
        "            X = np.array(X_by_state[state])\n",
        "            y = np.array(y_by_state[state])\n",
        "            if len(X) > 10:\n",
        "                x = 10\n",
        "                self.nn_models[state].fit(X, y)\n",
        "\n",
        "            # used to find the error from what the HMM predicted\n",
        "            for i, x_val in zip(y, X):\n",
        "                norm_i = i / (np.linalg.norm(i) + 1e-9)\n",
        "                norm_point = self.predict(x_val) / (np.linalg.norm(self.predict(x_val)) + 1e-9)\n",
        "                sum += scipy.spatial.distance.euclidean(norm_i, norm_point)\n",
        "                total += 1\n",
        "            print(total)\n",
        "            print(\"training loss: \", sum/total)\n",
        "\n",
        "        # updates starting transission matrix with the information learned from the training (used ot update the transimission matrix from being uniform to something that can accurately represent the data)\n",
        "        for i in range(len(states) - 1):\n",
        "            self.transition_matrix[states[i], states[i + 1]] += 1\n",
        "        self.transition_matrix /= self.transition_matrix.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # predict function\n",
        "    def predict(self, observations):\n",
        "        if observations.ndim == 1:\n",
        "            observations = [observations]\n",
        "        hidden_states = self.viterbi(observations)\n",
        "        last_hidden_state = hidden_states[-1]\n",
        "        return self.nn_models[last_hidden_state].predict([observations[-1]])[-1]\n",
        "\n",
        "    # defining the emission probabilities\n",
        "    def _emission_probabilities(self, observation):\n",
        "        emissions = []\n",
        "        for state in range(self.n_states):\n",
        "            try:\n",
        "                predicted = self.nn_models[state].predict([observation])\n",
        "                likelihood = np.exp(-np.linalg.norm(predicted - observation))\n",
        "            except Exception:\n",
        "                likelihood = 1e-6\n",
        "            emissions.append(likelihood)\n",
        "        emissions = np.array(emissions)\n",
        "        return emissions / emissions.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F-5QgkY0nLB-"
      },
      "outputs": [],
      "source": [
        "# Building LSTM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# outline of the LSTM class used\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size=3, hidden_size=128, num_layers=5, output_size=3):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        output = self.fc(out[:, :, :])\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FmsqMYZolJ8"
      },
      "source": [
        "preparing data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl9R_EhLonWB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# converting csv data to tensor\n",
        "all_observations = []\n",
        "\n",
        "# replace with folder for the train\n",
        "dataset_folder = \"/content/train\"\n",
        "datasets_list = os.listdir(dataset_folder)\n",
        "raw_data = []\n",
        "for dt in datasets_list:\n",
        "  raw_data = pd.read_csv(os.path.join(dataset_folder, dt), delimiter=\"\\t\",\n",
        "                               names=[\"frame\", \"ped\", \"x\", \"y\", \"z\"], usecols=[0, 1, 2, 3, 4], na_values=\"?\")\n",
        "  raw_data.sort_values(by=['frame', 'ped'], inplace=True)\n",
        "\n",
        "  ped_ids = raw_data.ped.unique()\n",
        "  for p in ped_ids:\n",
        "      all_observations.append(raw_data[raw_data.ped == p][[\"x\", \"y\", \"z\"]].values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6oMrsOSnyMH"
      },
      "source": [
        "Training the trajectory prediction models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bn6zg6u3nzX5"
      },
      "outputs": [],
      "source": [
        "# HMM\n",
        "# splitting data into train and test (80/20)\n",
        "random.shuffle(all_observations)\n",
        "n = len(all_observations)\n",
        "end = int(n*0.8)\n",
        "train_data = all_observations[:end]\n",
        "test_data = all_observations[end:]\n",
        "\n",
        "# defining model and training it\n",
        "HMM = HMM3DWithViterbi(n_states=4)\n",
        "max_epochs = 10\n",
        "cur_epoch = 0\n",
        "while(cur_epoch<max_epochs):\n",
        "  HMM.fit(train_data)\n",
        "  cur_epoch+= 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ldDuwnedoCBb"
      },
      "outputs": [],
      "source": [
        "# LSTM\n",
        "def create_tensor_dataset(data):\n",
        "    \"\"\" Convert list of numpy arrays to PyTorch tensors \"\"\"\n",
        "    X = [torch.tensor(traj[:-1], dtype=torch.float32) for traj in data]\n",
        "    Y = [torch.tensor(traj[1:], dtype=torch.float32) for traj in data]\n",
        "    return list(zip(X, Y))\n",
        "\n",
        "# Split into training (80%) and testing (20%) datasets\n",
        "random.shuffle(all_observations)\n",
        "train_data, test_data = train_test_split(all_observations, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize LSTM model\n",
        "lstm = LSTM(3, 128, 4, 3).to(device)\n",
        "\n",
        "# Convert list of numpy arrays to PyTorch tensors\n",
        "train_dataset = create_tensor_dataset(train_data)\n",
        "test_dataset = create_tensor_dataset(test_data)\n",
        "\n",
        "# Data loaders\n",
        "batch_size = 128\n",
        "tr_dl = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# Optimizer & Loss Function\n",
        "optimizer = Adam(lstm.parameters(), lr=0.001)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "# TensorBoard Logger\n",
        "log = SummaryWriter(f'logs/Ind_model')\n",
        "log.add_scalar('eval/mad', 0, 0)\n",
        "log.add_scalar('eval/fad', 0, 0)\n",
        "\n",
        "# Store losses for plotting later\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# Training Loop\n",
        "max_epoch = 50  # Set number of epochs\n",
        "for epoch in range(max_epoch):\n",
        "  lstm.train()\n",
        "  epoch_loss = 0\n",
        "\n",
        "  # Training phase\n",
        "  for X_batch, Y_batch in tr_dl:\n",
        "    optimizer.zero_grad()\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    pred = lstm(X_batch)\n",
        "\n",
        "    # Compute the Euclidean loss (norm of the difference)\n",
        "    loss = torch.mean(torch.norm(pred - Y_batch, dim=-1))\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  # Average training loss\n",
        "  avg_train_loss = epoch_loss / len(tr_dl)\n",
        "  print(f\"Epoch {epoch + 1}/{max_epoch}: Train Loss = {avg_train_loss:.4f}\")\n",
        "\n",
        "  # Append the training loss to the list\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "  # Log the training loss to TensorBoard\n",
        "  log.add_scalar('Loss/train', avg_train_loss, epoch + 1)\n",
        "\n",
        "  # Testing Phase within each epoch\n",
        "  lstm.eval()\n",
        "  test_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X_batch, Y_batch in test_dl:\n",
        "      X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "      # Make predictions on the test data\n",
        "      pred = lstm(X_batch)\n",
        "\n",
        "      # Compute the Euclidean loss for the test data\n",
        "      loss = torch.mean(torch.norm(pred - Y_batch, dim=-1))\n",
        "      test_loss += loss.item()\n",
        "\n",
        "  # Average test loss\n",
        "  avg_test_loss = test_loss / len(test_dl)\n",
        "  print(f\"Epoch {epoch + 1}/{max_epoch}: Test Loss = {avg_test_loss:.4f}\")\n",
        "\n",
        "  # Append the test loss to the list\n",
        "  test_losses.append(avg_test_loss)\n",
        "\n",
        "  # Log the test loss to TensorBoard\n",
        "  log.add_scalar('Loss/test', avg_test_loss, epoch + 1)\n",
        "\n",
        "# # Plot the training and test losses\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(range(1, max_epoch + 1), train_losses, label='Train Loss', color='blue')\n",
        "# plt.plot(range(1, max_epoch + 1), test_losses, label='Test Loss', color='red')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.title('Training and Test Loss Over Epochs')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.savefig('loss_plot.png')  # Save the plot to a file\n",
        "# plt.show()  # Display the plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoQFl5OYoCYc"
      },
      "source": [
        "OR insert previously trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOstOeKhoFI1"
      },
      "outputs": [],
      "source": [
        "# bring in models\n",
        "# HMM\n",
        "import pickle\n",
        "HMM = pickle.load(open('/content/models/hmm3d_model.pkl', 'rb')) # replace with location of files\n",
        "\n",
        "# LSTM\n",
        "import torch\n",
        "lstm = LSTM(3, 128, 4, 3)\n",
        "lstm.load_state_dict(torch.load('/content/models/trajectory_lstm.pth')) # replace with location of files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRrDbkZaqT-7"
      },
      "source": [
        "Build Hybrid Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "geBmBTPcqqq9",
        "outputId": "5bc75176-0bc0-42f8-bb68-0126e8e7eed4"
      },
      "outputs": [],
      "source": [
        "ped = raw_data.ped.unique()\n",
        "\n",
        "np.random.seed(52)\n",
        "random.seed(42)\n",
        "\n",
        "# Collect predictions from each model in a list\n",
        "predictions_lstm = []\n",
        "predictions_hmm = []\n",
        "predictions_TF = []\n",
        "outputs = []\n",
        "pos = raw_data[raw_data.ped == ped[1]].to_numpy()\n",
        "\n",
        "test = []\n",
        "true = []\n",
        "device = torch.device(\"cpu\")\n",
        "for p in ped:\n",
        "    random_number = random.randint(1, 19)\n",
        "    pos = raw_data[raw_data.ped == ped[random_number]].to_numpy()\n",
        "    start = pos[0, 2:5]\n",
        "    test = pos[1:, 2:5]\n",
        "    his = [start]  # Initialize history with the first position\n",
        "\n",
        "    for i in test:\n",
        "        true.append(i)\n",
        "        his_tensor = torch.tensor(his, dtype=torch.float32)\n",
        "        his_tensor_lstm = torch.tensor([his], dtype=torch.float32)\n",
        "\n",
        "        # Get predictions from each model\n",
        "        pred_lstm = lstm(his_tensor_lstm).detach().numpy()\n",
        "        pred_hmm = np.array(HMM.predict(his_tensor))\n",
        "\n",
        "        # Append predictions to respective lists\n",
        "        predictions_lstm.append(pred_lstm)\n",
        "        predictions_hmm.append(pred_hmm)\n",
        "\n",
        "        # Append new position to history\n",
        "        his.append(i)\n",
        "        outputs.append(i)\n",
        "        break\n",
        "\n",
        "\n",
        "# reshape prediction arrays to be the same size\n",
        "predictions_lstm = np.array(predictions_lstm).reshape(-1, 3)\n",
        "predictions_hmm = np.array(predictions_hmm).reshape(-1, 3)\n",
        "\n",
        "# stacking the predictions by x, y, z\n",
        "X_stack = np.hstack([predictions_lstm[:, 0].reshape(-1, 1), predictions_hmm[:, 0].reshape(-1, 1)])  # Shape: (num_samples, 9)\n",
        "Y_stack = np.hstack([predictions_lstm[:, 1].reshape(-1, 1), predictions_hmm[:, 1].reshape(-1, 1)])\n",
        "Z_stack = np.hstack([predictions_lstm[:, 2].reshape(-1, 1), predictions_hmm[:, 2].reshape(-1, 1)])\n",
        "\n",
        "# splitting the ground truth into x, y, z\n",
        "true = np.array(true)\n",
        "y_true = torch.tensor(true[:, 1]).unsqueeze(1).numpy()\n",
        "z_true = torch.tensor(true[:, 2]).unsqueeze(1).numpy()\n",
        "x_true = torch.tensor(true[:, 0]).unsqueeze(1).numpy()\n",
        "\n",
        "# train the respective random forest regressor (x, y, z)\n",
        "meta_model_x = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "meta_model_x.fit(X_stack, x_true)\n",
        "\n",
        "meta_model_y = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "meta_model_y.fit(Y_stack, y_true)\n",
        "\n",
        "meta_model_z = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "meta_model_z.fit(Z_stack, z_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRHWW8YyrOsl"
      },
      "source": [
        "or import previously trained hybrid models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm_XA6mTrQfp"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# replace file names with new location\n",
        "with open(\"traj_hybrid_x.pkl\", \"wb\") as f:\n",
        "    pickle.dump(meta_model_x, f)\n",
        "with open(\"traj_hybrid_y.pkl\", \"wb\") as f:\n",
        "    pickle.dump(meta_model_y, f)\n",
        "with open(\"traj_hybrid_z.pkl\", \"wb\") as f:\n",
        "    pickle.dump(meta_model_z, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9_kqx2Cri1U"
      },
      "source": [
        "Compare the trajectory models (LSTM, HMM, and Hybrid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "id": "Ns6W7EIgrle4",
        "outputId": "57230fab-b209-4003-d214-1fef2aabc6d3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to calculate 3D Euclidean distance error\n",
        "def calculate_3d_error(predictions, true_values):\n",
        "    return np.linalg.norm(predictions - true_values, axis=1)\n",
        "\n",
        "ped = raw_data.ped.unique()\n",
        "for p in ped:\n",
        "  traj1 = raw_data[raw_data.ped == p].to_numpy()[:, 2:5]\n",
        "  start = np.array(traj1[0])\n",
        "  rest = traj1[0:]\n",
        "  his = [start]\n",
        "\n",
        "  # Error tracking lists\n",
        "  avg_lstm_error = []\n",
        "  avg_hmm_error = []\n",
        "  avg_hybrid_error = []\n",
        "\n",
        "  max_lstm_error = 0\n",
        "  max_hmm_error = 0\n",
        "  max_hybrid_error = 0\n",
        "\n",
        "  fig = plt.figure(figsize=(10, 8))\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "  count = 0\n",
        "\n",
        "\n",
        "  for point in rest:\n",
        "      # Convert history to tensors\n",
        "      his_tensor = torch.tensor(his, dtype=torch.float32)\n",
        "      his_tensor_lstm = torch.tensor([his], dtype=torch.float32)\n",
        "\n",
        "      # Get predictions from each model\n",
        "      pred_lstm = lstm(his_tensor_lstm).detach().numpy()[:, 0]\n",
        "      pred_hmm = np.array(HMM.predict(his_tensor)).reshape(1, 3)\n",
        "\n",
        "      # Hybrid Model Predictions\n",
        "      X_stack = np.hstack([pred_lstm[:, 0].reshape(-1, 1), pred_hmm[:, 0].reshape(-1, 1)])\n",
        "      Y_stack = np.hstack([pred_lstm[:, 1].reshape(-1, 1), pred_hmm[:, 1].reshape(-1, 1)])\n",
        "      Z_stack = np.hstack([pred_lstm[:, 2].reshape(-1, 1), pred_hmm[:, 2].reshape(-1, 1)])\n",
        "\n",
        "      pred_x = meta_model_x.predict([X_stack[-1]])\n",
        "      pred_y = meta_model_y.predict([Y_stack[-1]])\n",
        "      pred_z = meta_model_z.predict([Z_stack[-1]])\n",
        "\n",
        "      pred_hybrid = np.column_stack((pred_x, pred_y, pred_z))\n",
        "\n",
        "      # Calculate 3D error\n",
        "      lstm_error = calculate_3d_error(pred_lstm, point)\n",
        "      hmm_error = calculate_3d_error(pred_hmm, point)\n",
        "      hybrid_error = calculate_3d_error(pred_hybrid[0], np.array([point]))\n",
        "\n",
        "      # Store errors\n",
        "      avg_lstm_error.append(lstm_error[0])\n",
        "      avg_hmm_error.append(hmm_error[0])\n",
        "      avg_hybrid_error.append(hybrid_error[0])\n",
        "\n",
        "      # Update maximum errors\n",
        "      max_lstm_error = max(max_lstm_error, lstm_error[0])\n",
        "      max_hmm_error = max(max_hmm_error, hmm_error[0])\n",
        "      max_hybrid_error = max(max_hybrid_error, hybrid_error[0])\n",
        "\n",
        "      his.append(point)\n",
        "\n",
        "      # # 3D Scatter Plot\n",
        "\n",
        "      # colors = sns.color_palette(\"husl\")\n",
        "      # if(count % 3 == 0):\n",
        "      #   ax.scatter(pred_lstm[:, 0], pred_lstm[:, 1], pred_lstm[:, 2],\n",
        "      #         color='slateblue', alpha=1, marker=\"o\", s=40)\n",
        "\n",
        "      #   ax.scatter(pred_hmm[:, 0], pred_hmm[:, 1], pred_hmm[:, 2],\n",
        "      #             color='green', alpha=1, marker='^', s=40)\n",
        "\n",
        "      #   ax.scatter(pred_hybrid[:, 0], pred_hybrid[:, 1], pred_hybrid[:, 2],\n",
        "      #         color='#ff073a', alpha=1.0, marker='*', s=100, zorder=5)\n",
        "      # count += 1\n",
        "\n",
        "      # ax.set_xlabel('X')\n",
        "      # ax.set_ylabel('Y')\n",
        "      # ax.set_zlabel('Z')\n",
        "      # ax.set_title('3D Prediction vs Actual Points')\n",
        "\n",
        "  # # continued plotting\n",
        "  # his = np.array(his)\n",
        "  # ax.plot(his[:, 0], his[:, 1], his[:, 2], color='black', alpha=1, linewidth = 3, label=\"Actual Points\", zorder=4)\n",
        "\n",
        "  # ax.xaxis._axinfo[\"grid\"].update(color=\"lightgray\", linestyle=\"dashed\", linewidth=0.5)\n",
        "  # ax.yaxis._axinfo[\"grid\"].update(color=\"lightgray\", linestyle=\"dashed\", linewidth=0.5)\n",
        "  # ax.zaxis._axinfo[\"grid\"].update(color=\"lightgray\", linestyle=\"dashed\", linewidth=0.5)\n",
        "\n",
        "  # ax.scatter([],[],[],\n",
        "  #             color='slateblue', alpha=1, marker=\"o\", s=40, label=\"LSTM Prediction\", zorder=1)\n",
        "\n",
        "  # ax.scatter([],[],[],\n",
        "  #             color='green', alpha=1, marker='^', s=40, label=\"HMM Prediction\", zorder=2)\n",
        "\n",
        "  # ax.scatter([],[],[],\n",
        "  #             color='#ff073a', alpha=1.0, marker='*', s=100, zorder=5, label=\"Hybrid Prediction\")\n",
        "\n",
        "  # ax.legend()\n",
        "  # plt.show()\n",
        "\n",
        "  # # Final error reporting\n",
        "  # print(\"\\nFinal Error Summary:\")\n",
        "  # print(f\"LSTM Average 3D Error: {np.mean(avg_lstm_error):.4f}, Maximum 3D Error: {max_lstm_error:.4f}\")\n",
        "  # print(f\"HMM Average 3D Error: {np.mean(avg_hmm_error):.4f}, Maximum 3D Error: {max_hmm_error:.4f}\")\n",
        "  # print(f\"Hybrid Model Average 3D Error: {np.mean(avg_hybrid_error):.4f}, Maximum 3D Error: {max_hybrid_error:.4f}\")\n",
        "\n",
        "# # more plotting\n",
        "# # Initialize 3D Scatter Plot before the loop\n",
        "# fig = plt.figure(figsize=(10, 8))\n",
        "# ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# ax.set_xlabel('X')\n",
        "# ax.set_ylabel('Y')\n",
        "# ax.set_zlabel('Z')\n",
        "# ax.set_title('3D Prediction vs Actual Points')\n",
        "\n",
        "# # Initialize empty lists for storing scatter plot points\n",
        "# actual_points = []\n",
        "# lstm_points = []\n",
        "# hmm_points = []\n",
        "# hybrid_points = []\n",
        "\n",
        "# for point in rest:\n",
        "#     # Convert history to tensors\n",
        "#     his_tensor = torch.tensor(his, dtype=torch.float32)\n",
        "#     his_tensor_lstm = torch.tensor([his], dtype=torch.float32)\n",
        "\n",
        "#     # Get predictions\n",
        "#     pred_lstm = lstm(his_tensor_lstm).detach().numpy()\n",
        "#     pred_hmm = np.array(HMM.predict(his_tensor)).reshape(1, 3)\n",
        "\n",
        "#     # Hybrid Predictions\n",
        "#     X_stack = np.hstack([pred_lstm[:, 0].reshape(-1, 1), pred_hmm[:, 0].reshape(-1, 1)])\n",
        "#     Y_stack = np.hstack([pred_lstm[:, 1].reshape(-1, 1), pred_hmm[:, 1].reshape(-1, 1)])\n",
        "#     Z_stack = np.hstack([pred_lstm[:, 2].reshape(-1, 1), pred_hmm[:, 2].reshape(-1, 1)])\n",
        "\n",
        "#     pred_x = meta_model_x.predict([X_stack[-1]])\n",
        "#     pred_y = meta_model_y.predict([Y_stack[-1]])\n",
        "#     pred_z = meta_model_z.predict([Z_stack[-1]])\n",
        "\n",
        "#     pred_hybrid = np.column_stack((pred_x, pred_y, pred_z))\n",
        "\n",
        "#     # Store points\n",
        "#     actual_points.append(point)\n",
        "#     lstm_points.append(pred_lstm[0])\n",
        "#     hmm_points.append(pred_hmm[0])\n",
        "#     hybrid_points.append(pred_hybrid[0])\n",
        "\n",
        "# # Convert lists to numpy arrays for easier plotting\n",
        "# actual_points = np.array(actual_points)\n",
        "# lstm_points = np.array(lstm_points)\n",
        "# hmm_points = np.array(hmm_points)\n",
        "# hybrid_points = np.array(hybrid_points)\n",
        "\n",
        "# # Scatter all points in the same figure\n",
        "# ax.scatter(actual_points[:, 0], actual_points[:, 1], actual_points[:, 2], color='black', marker='o', s=50, label='Actual')\n",
        "\n",
        "# fig = plt.figure(figsize=(10, 8))\n",
        "# ax = fig.add_subplot(111, projection='3d')\n",
        "# ax.scatter(lstm_points[0, 0], lstm_points[0, 1], lstm_points[0, 2], color='red', marker='^', s=50, label='LSTM')\n",
        "# ax.scatter(lstm_points[1, 0], lstm_points[1, 1], lstm_points[1, 2], color='red', marker='^', s=50, label='LSTM')\n",
        "# ax.scatter(hmm_points[:, 0], hmm_points[:, 1], hmm_points[:, 2], color='blue', marker='s', s=50, label='HMM')\n",
        "# ax.scatter(hybrid_points[:, 0], hybrid_points[:, 1], hybrid_points[:, 2], color='orange', marker='*', s=50, label='Hybrid')\n",
        "\n",
        "# ax.legend()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRxQu_GK1D4G"
      },
      "source": [
        "Combine source approximation and hybrid model into the complete predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "HlBvxWvJsCQV",
        "outputId": "4bca3ee1-bf7b-4e29-d866-d5c95b3d4067"
      },
      "outputs": [],
      "source": [
        "# integrating the source approximation\n",
        "# Constants for Gaussian plume model\n",
        "Q, u, ay, by, az, bz, H = 1, 1, 0.5, 0.92, 0.5, 0.87, 1\n",
        "\n",
        "# Euclidean distance loss function\n",
        "\n",
        "def generate_random_points(x0, y0, z0):\n",
        "    x = [0, 0, 0]\n",
        "    y = [0, 0, 0]\n",
        "    z = [0, 0, 0]\n",
        "\n",
        "    for i in range(3):\n",
        "        x[i] = x0 + np.random.uniform(0,1)\n",
        "    for i in range(3):\n",
        "        y[i] = y0 + np.random.uniform(0,1)\n",
        "    for i in range(3):\n",
        "        z[i] = z0 + np.random.uniform(0.01,0.1)\n",
        "\n",
        "    return [[x[0], y[0], z[0]], [x[1], y[1], z[1]], [x[2], y[2], z[2]]]\n",
        "\n",
        "# Function to generate synthetic data\n",
        "def generate_data(sources):\n",
        "    X_train, Y_train = [], []\n",
        "    sources = np.array(sources)\n",
        "\n",
        "    for x0, y0, z0 in sources:\n",
        "        points = generate_random_points(x0, y0, z0)\n",
        "        concentrations = []\n",
        "\n",
        "        for x, y, z in points:\n",
        "\n",
        "            # Constants\n",
        "            x_factor = 2\n",
        "            u = 1  # Wind speed\n",
        "            H = 0\n",
        "            ay, by = 1, 0.92\n",
        "            az, bz = 1, 0.87\n",
        "\n",
        "            # Calculate s_y and s_z\n",
        "            s_y = 1 * ay * abs(x - x0) ** by\n",
        "            s_z = 1 * az * abs(x - x0) ** bz\n",
        "\n",
        "            # Take care of divide by zero by checking s_y and s_z\n",
        "            if s_y == 0:\n",
        "                s_y = 0.0002\n",
        "            if s_z == 0:\n",
        "                s_z = 0.0002\n",
        "\n",
        "            # Calculate the exponential terms\n",
        "            EXP1 = -(y - y0) ** 2 / (s_y ** 2)\n",
        "            EXP2 = -(z - z0 - H) ** 2 / (2 * s_z ** 2)\n",
        "            EXP3 = -(z - z0 + H) ** 2 / (2 * s_z ** 2)\n",
        "\n",
        "            # Calculate concentration\n",
        "            concentration = 100 * 1 / (2 * np.pi * s_y * s_z * u)\n",
        "\n",
        "            # Check if x is less than x0 (if so, concentration becomes 0)\n",
        "            if x < x0:\n",
        "                concentration = 0\n",
        "\n",
        "            # Calculate concentration based on the exponential terms\n",
        "            concentration *= np.exp(EXP1)\n",
        "            concentration *= np.exp(EXP2) + np.exp(EXP3)\n",
        "\n",
        "            # Max concentration to 300\n",
        "            if concentration > 300:\n",
        "                concentration = 300\n",
        "\n",
        "\n",
        "            concentrations.append(concentration)\n",
        "\n",
        "        input_features = np.column_stack((points, concentrations))\n",
        "        X_train.append(input_features)\n",
        "        Y_train.append([x0, y0, z0])\n",
        "    return np.array(X_train), np.array(Y_train).tolist()\n",
        "\n",
        "# Generate and split data\n",
        "\n",
        "# define the source model and define way to get the concentration readings\n",
        "def euclidean_distance_loss(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_sum(tf.square(y_true - y_pred), axis=1))\n",
        "\n",
        "# Function to calculate 3D Euclidean distance error\n",
        "def calculate_3d_error(predictions, true_values):\n",
        "    return np.linalg.norm(predictions - true_values, axis=1)\n",
        "\n",
        "\n",
        "ped = raw_data.ped.unique()\n",
        "for p in ped:\n",
        "  traj1 = raw_data[raw_data.ped == p].to_numpy()[:, 2:5]\n",
        "  start = np.array(traj1[0])\n",
        "  rest = traj1[0:]\n",
        "  x, y = generate_data([start])\n",
        "  his = source.predict(x).tolist()\n",
        "\n",
        "  # Error tracking lists\n",
        "  avg_lstm_error = []\n",
        "  avg_hmm_error = []\n",
        "  avg_hybrid_error = []\n",
        "\n",
        "  max_lstm_error = 0\n",
        "  max_hmm_error = 0\n",
        "  max_hybrid_error = 0\n",
        "\n",
        "  fig = plt.figure(figsize=(10, 8))\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "\n",
        "  for point in rest:\n",
        "      # Convert history to tensors\n",
        "      his_tensor = torch.tensor(his, dtype=torch.float32)\n",
        "      his_tensor_lstm = torch.tensor([his], dtype=torch.float32)\n",
        "\n",
        "      # Get predictions from each model\n",
        "      pred_lstm = lstm(his_tensor_lstm).detach().numpy()[:, 0]\n",
        "      pred_hmm = np.array(HMM.predict(his_tensor)).reshape(1, 3)\n",
        "\n",
        "      # Hybrid Model Predictions\n",
        "      X_stack = np.hstack([pred_lstm[:, 0].reshape(-1, 1), pred_hmm[:, 0].reshape(-1, 1)])\n",
        "      Y_stack = np.hstack([pred_lstm[:, 1].reshape(-1, 1), pred_hmm[:, 1].reshape(-1, 1)])\n",
        "      Z_stack = np.hstack([pred_lstm[:, 2].reshape(-1, 1), pred_hmm[:, 2].reshape(-1, 1)])\n",
        "\n",
        "      pred_x = meta_model_x.predict([X_stack[-1]])\n",
        "      pred_y = meta_model_y.predict([Y_stack[-1]])\n",
        "      pred_z = meta_model_z.predict([Z_stack[-1]])\n",
        "\n",
        "      pred_hybrid = np.column_stack((pred_x, pred_y, pred_z))\n",
        "\n",
        "      # Calculate 3D error\n",
        "      lstm_error = calculate_3d_error(pred_lstm, point)\n",
        "      hmm_error = calculate_3d_error(pred_hmm, point)\n",
        "      hybrid_error = calculate_3d_error(pred_hybrid[0], np.array([point]))\n",
        "\n",
        "      # Store errors\n",
        "      avg_lstm_error.append(lstm_error[0])\n",
        "      avg_hmm_error.append(hmm_error[0])\n",
        "      avg_hybrid_error.append(hybrid_error[0])\n",
        "\n",
        "      # Update maximum errors\n",
        "      max_lstm_error = max(max_lstm_error, lstm_error[0])\n",
        "      max_hmm_error = max(max_hmm_error, hmm_error[0])\n",
        "      max_hybrid_error = max(max_hybrid_error, hybrid_error[0])\n",
        "\n",
        "      # 3D Scatter Plot\n",
        "\n",
        "      # colors = sns.color_palette(\"husl\")\n",
        "\n",
        "      # ax.scatter(pred_hybrid[:, 0], pred_hybrid[:, 1], pred_hybrid[:, 2],\n",
        "      #       color='red', alpha=1.0, marker='*', s=45, zorder=5)\n",
        "\n",
        "      # ax.set_xlabel('X')\n",
        "      # ax.set_ylabel('Y')\n",
        "      # ax.set_zlabel('Z')\n",
        "      # ax.set_title('3D Prediction vs Actual Points')\n",
        "\n",
        "      x, y = generate_data([point])\n",
        "      his.append(source.predict(x)[0].tolist())\n",
        "\n",
        "  # # continued plotting\n",
        "  # ax.plot(rest[:, 0], rest[:, 1], rest[:, 2], color='black', alpha=1, linewidth = 3, label=\"Actual Points\", zorder=4)\n",
        "\n",
        "  # ax.xaxis._axinfo[\"grid\"].update(color=\"lightgray\", linestyle=\"dashed\", linewidth=0.5)\n",
        "  # ax.yaxis._axinfo[\"grid\"].update(color=\"lightgray\", linestyle=\"dashed\", linewidth=0.5)\n",
        "  # ax.zaxis._axinfo[\"grid\"].update(color=\"lightgray\", linestyle=\"dashed\", linewidth=0.5)\n",
        "\n",
        "  # ax.scatter([],[],[],\n",
        "  #               color=sns.color_palette(\"dark\")[4], alpha=0.6, label=f'LSTM Prediction', marker='|', s=20, zorder=1)\n",
        "  # ax.scatter([],[],[],\n",
        "  #               color=sns.color_palette(\"dark\")[3], alpha=0.6, label=f'HMM Prediction', marker='_', s=20, zorder=3)\n",
        "  # ax.scatter([],[],[],\n",
        "  #               color=sns.color_palette(\"bright\")[8], alpha=1, label=f'Hybrid Prediction', marker='*', s=37, zorder=2)\n",
        "  # ax.scatter([],[],[],\n",
        "  #               color='black', label='Actual Points', marker='o', s=40, zorder=4)\n",
        "  # plt.show()\n",
        "\n",
        "  # # Final error reporting\n",
        "  # print(\"\\nFinal Error Summary:\")\n",
        "  # print(f\"LSTM Average 3D Error: {np.mean(avg_lstm_error):.4f}, Maximum 3D Error: {max_lstm_error:.4f}\")\n",
        "  # print(f\"HMM Average 3D Error: {np.mean(avg_hmm_error):.4f}, Maximum 3D Error: {max_hmm_error:.4f}\")\n",
        "  # print(f\"Hybrid Model Average 3D Error: {np.mean(avg_hybrid_error):.4f}, Maximum 3D Error: {max_hybrid_error:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
